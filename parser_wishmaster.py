import sqlite3
from datetime import datetime

import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
categories = {
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_pro/": "Apple iPhone 16 Pro",
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_plus/": "Apple iPhone 16 Plus",
}


# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
def create_database():
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            category TEXT,
            name TEXT,
            price TEXT,
            price_difference TEXT,
            stock_status TEXT,
            timestamp TEXT
        )
    """)

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_name ON products (name);")

    conn.commit()
    conn.close()


# –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É —Ç–æ–≤–∞—Ä–∞
def get_last_price(cursor, name):
    cursor.execute("SELECT price FROM products WHERE name = ? ORDER BY timestamp DESC LIMIT 1", (name,))
    result = cursor.fetchone()
    return result[0] if result else None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É
def save_to_db(category, products):
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    for product in products:
        name, new_price, stock_status = product

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ü–µ–Ω—É
        old_price = get_last_price(cursor, name)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É
        if old_price and old_price != new_price:
            try:
                price_diff = float(new_price.replace(" ", "").replace("‚ÇΩ", "")) - float(
                    old_price.replace(" ", "").replace("‚ÇΩ", ""))
                price_diff = f"{price_diff:+,.2f} ‚ÇΩ".replace(",", " ")  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —á–∏—Ç–∞–µ–º–æ–º—É –≤–∏–¥—É
            except ValueError:
                price_diff = "–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"
        else:
            price_diff = "0 ‚ÇΩ" if old_price else "–ù–æ–≤—ã–π —Ç–æ–≤–∞—Ä"

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –≤ –ë–î (—Å—Ç–∞—Ä–∞—è –æ—Å—Ç–∞–µ—Ç—Å—è)
        cursor.execute("""
            INSERT INTO products (category, name, price, price_difference, stock_status, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (category, name, new_price, price_diff, stock_status, datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø–∏—Å—å: {name} | –¶–µ–Ω–∞: {new_price} | –†–∞–∑–Ω–∏—Ü–∞: {price_diff}")

    conn.commit()
    conn.close()


# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
def get_pagination_links(base_url):
    response = requests.get(base_url, headers=HEADERS)
    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {base_url}")
        return [base_url]

    soup = BeautifulSoup(response.content, "html.parser")
    pagination = soup.find("div", class_="catalog-pagination__nums")

    if not pagination:
        print(f"‚ÑπÔ∏è –ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {base_url}. –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        return [base_url]

    links = pagination.find_all("a", href=True)
    page_numbers = []

    for link in links:
        href = link["href"]
        if "PAGEN_2=" in href:
            try:
                page_numbers.append(int(href.split("PAGEN_2=")[-1]))
            except ValueError:
                continue

    max_page = max(page_numbers) if page_numbers else 1
    return [f"{base_url}?PAGEN_2={page}" for page in range(1, max_page + 1)]


# –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
def parse_wishmaster(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {url}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")

    names = soup.find_all("div", class_="catalog-rounded-item__name")
    prices = soup.find_all("span", class_="catalog-rounded-item__price")
    stock_statuses = soup.find_all("div", class_="catalog-rounded-item__quantity-text")

    if not names or not prices or not stock_statuses:
        print(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
        return []

    products = [
        (name.text.strip(), price.text.strip(), stock.text.strip())
        for name, price, stock in zip(names, prices, stock_statuses)
    ]

    print(f"üîé –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)} –Ω–∞ {url}")
    return products


# –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
def parse_category(category_url, category_name):
    print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_name} ({category_url})")
    pages = get_pagination_links(category_url)
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    all_products = []
    for index, url in enumerate(pages, start=1):
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {index} ‚Üí {url}")
        products = parse_wishmaster(url)
        all_products.extend(products)

    save_to_db(category_name, all_products)


# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫
if __name__ == "__main__":
    create_database()
    for url, category in categories.items():
        parse_category(url, category)

    print("\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑–µ!")
