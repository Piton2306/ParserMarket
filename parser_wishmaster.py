import sqlite3
from datetime import datetime

import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ)
categories = {
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_pro/": "Apple iPhone 16 Pro",
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_plus/": "Apple iPhone 16 Plus",
}


# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü—ã
def create_database():
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            category TEXT,
            name TEXT,
            price TEXT,
            stock_status TEXT,
            timestamp TEXT
        )
    """)

    conn.commit()
    conn.close()


def save_to_db(category, products):
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    for product in products:
        cursor.execute("""
            INSERT INTO products (category, name, price, stock_status, timestamp)
            VALUES (?, ?, ?, ?, ?)
        """, (category, product[0], product[1], product[2], datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

    conn.commit()
    conn.close()


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ (href) –≤ –±–ª–æ–∫–µ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
def get_pagination_links(base_url):
    response = requests.get(base_url, headers=HEADERS)

    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {base_url}: {response.status_code}")
        return [base_url]  # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞, –ø–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É

    soup = BeautifulSoup(response.content, "html.parser")

    # –ò—â–µ–º –±–ª–æ–∫ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    pagination = soup.find("div", class_="catalog-pagination__nums")

    if not pagination:
        print(f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {base_url}. –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        return [base_url]

    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    links = pagination.find_all("a", href=True)

    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    page_numbers = []
    for link in links:
        href = link["href"]
        if "PAGEN_2=" in href:
            try:
                page_num = int(href.split("PAGEN_2=")[-1])  # –î–æ—Å—Ç–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_numbers.append(page_num)
            except ValueError:
                continue  # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    if not page_numbers:
        return [base_url]  # –ï—Å–ª–∏ –Ω–µ—Ç –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è

    max_page = max(page_numbers)  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –æ—Ç 1 –¥–æ max_page
    all_pages = [base_url]  # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    for page in range(2, max_page + 1):
        all_pages.append(f"{base_url}?PAGEN_2={page}")

    return all_pages


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤ (–Ω–∞–∑–≤–∞–Ω–∏–µ, —Ü–µ–Ω–∞, –Ω–∞–ª–∏—á–∏–µ)
def parse_wishmaster(url):
    response = requests.get(url, headers=HEADERS)

    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {url}: {response.status_code}")
        return []  # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É

    soup = BeautifulSoup(response.content, "html.parser")

    # –ò—â–µ–º —Ç–æ–≤–∞—Ä—ã
    names = soup.find_all("div", class_="catalog-rounded-item__name")
    prices = soup.find_all("span", class_="catalog-rounded-item__price")
    stock_statuses = soup.find_all("div", class_="catalog-rounded-item__quantity-text")

    if not names or not prices or not stock_statuses:
        print(f"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
        return []  # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤

    products = []
    for name, price, stock in zip(names, prices, stock_statuses):
        product_name = name.text.strip()
        product_price = price.text.strip()
        stock_text = stock.text.strip()

        products.append((product_name, product_price, stock_text))

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")
    return products


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
def parse_category(category_url, category_name):
    print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_name} ({category_url})")
    pages = get_pagination_links(category_url)
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    all_products = []
    for index, url in enumerate(pages, start=1):
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {index}: {url}")

        products = parse_wishmaster(url)
        all_products.extend(products)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
    save_to_db(category_name, all_products)
    return all_products


# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
create_database()

# –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
all_category_products = {}

# –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
for url, category_name in categories.items():
    parse_category(url, category_name)

print("\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
