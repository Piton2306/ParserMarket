import sqlite3
from datetime import datetime

import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ)
categories = {
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_pro/": "Apple iPhone 16 Pro",
    "https://wishmaster.me/catalog/smartfony/smartfony_apple/iphone_16_plus/": "Apple iPhone 16 Plus",
}


# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü—ã
def create_database():
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            category TEXT,
            name TEXT,
            price TEXT,
            stock_status TEXT,
            timestamp TEXT
        )
    """)

    conn.commit()
    conn.close()


# –ü—Ä–æ–≤–µ—Ä–∫–∞, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ —Ü–µ–Ω–∞
def is_price_changed(cursor, name, new_price):
    cursor.execute("SELECT price FROM products WHERE name = ?", (name,))
    result = cursor.fetchone()
    if result:
        return result[0] != new_price  # True, –µ—Å–ª–∏ —Ü–µ–Ω–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å
    return True  # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä–∞ –Ω–µ—Ç –≤ –±–∞–∑–µ, –¥–æ–±–∞–≤–ª—è–µ–º


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ü–µ–Ω—ã
def save_to_db(category, products):
    conn = sqlite3.connect("wishmaster.db")
    cursor = conn.cursor()

    for product in products:
        name, price, stock_status = product

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ —Ü–µ–Ω–∞
        if is_price_changed(cursor, name, price):
            cursor.execute("""
                INSERT INTO products (category, name, price, stock_status, timestamp)
                VALUES (?, ?, ?, ?, ?)
            """, (category, name, price, stock_status, datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

    conn.commit()
    conn.close()


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ (href) –≤ –±–ª–æ–∫–µ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
def get_pagination_links(base_url):
    response = requests.get(base_url, headers=HEADERS)

    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {base_url}: {response.status_code}")
        return [base_url]  # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞, –ø–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É

    soup = BeautifulSoup(response.content, "html.parser")

    pagination = soup.find("div", class_="catalog-pagination__nums")

    if not pagination:
        print(f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {base_url}. –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        return [base_url]

    links = pagination.find_all("a", href=True)

    page_numbers = []
    for link in links:
        href = link["href"]
        if "PAGEN_2=" in href:
            try:
                page_num = int(href.split("PAGEN_2=")[-1])
                page_numbers.append(page_num)
            except ValueError:
                continue

    if not page_numbers:
        return [base_url]

    max_page = max(page_numbers)

    all_pages = [base_url]
    for page in range(2, max_page + 1):
        all_pages.append(f"{base_url}?PAGEN_2={page}")

    return all_pages


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤
def parse_wishmaster(url):
    response = requests.get(url, headers=HEADERS)

    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ {url}: {response.status_code}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")

    names = soup.find_all("div", class_="catalog-rounded-item__name")
    prices = soup.find_all("span", class_="catalog-rounded-item__price")
    stock_statuses = soup.find_all("div", class_="catalog-rounded-item__quantity-text")

    if not names or not prices or not stock_statuses:
        print(f"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
        return []

    products = []
    for name, price, stock in zip(names, prices, stock_statuses):
        product_name = name.text.strip()
        product_price = price.text.strip()
        stock_text = stock.text.strip()

        products.append((product_name, product_price, stock_text))

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")
    return products


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
def parse_category(category_url, category_name):
    print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_name} ({category_url})")
    pages = get_pagination_links(category_url)
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    all_products = []
    for index, url in enumerate(pages, start=1):
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {index}: {url}")

        products = parse_wishmaster(url)
        all_products.extend(products)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
    save_to_db(category_name, all_products)
    return all_products


# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
create_database()

# –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
for url, category_name in categories.items():
    parse_category(url, category_name)

print("\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
