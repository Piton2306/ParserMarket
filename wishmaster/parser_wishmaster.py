import logging
import sqlite3
import time
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.py
from config import HEADERS, categories, DELAY_BETWEEN_REQUESTS, DATABASE_PATH, LOG_FILE_PATH

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE_PATH, encoding="utf-8"),  # –õ–æ–≥–∏ –≤ —Ñ–∞–π–ª
        logging.StreamHandler()  # –õ–æ–≥–∏ –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)


def create_database():
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS products (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                name TEXT,
                price TEXT,
                price_difference TEXT,
                stock_status TEXT,
                timestamp TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
    finally:
        conn.close()


def get_last_price(cursor, name):
    try:
        cursor.execute("SELECT price FROM products WHERE name = ? ORDER BY timestamp DESC LIMIT 1", (name,))
        result = cursor.fetchone()
        return result[0] if result else None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ü–µ–Ω—ã: {e}")
        return None


def save_to_db(category, products):
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        for product in products:
            name, new_price, stock_status = product

            # –û—á–∏—Å—Ç–∫–∞ —Ü–µ–Ω—ã –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç—ã
            clean_new_price = (
                new_price
                .replace("\u00A0", "")  # –£–¥–∞–ª—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                .replace(" ", "")  # –£–¥–∞–ª—è–µ–º –æ–±—ã—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                .replace("—Ä—É–±.", "")  # –£–¥–∞–ª—è–µ–º —Å–∏–º–≤–æ–ª –≤–∞–ª—é—Ç—ã
                .replace(",", ".")  # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
            )

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            old_price = get_last_price(cursor, name)

            # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ –∏ —Ü–µ–Ω–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if old_price:
                clean_old_price = (
                    old_price
                    .replace("\u00A0", "")
                    .replace(" ", "")
                    .replace("—Ä—É–±.", "")
                    .replace(",", ".")
                )
                if clean_new_price == clean_old_price:
                    logging.info(f"üîπ –¶–µ–Ω–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –¥–ª—è {name}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ —Ü–µ–Ω–µ
            if old_price:
                try:
                    price_difference = float(clean_new_price) - float(clean_old_price)
                except ValueError as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ä–∞–∑–Ω–∏—Ü—ã —Ü–µ–Ω –¥–ª—è {name}: {e}")
                    price_difference = "–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"
            else:
                price_difference = "–ù–æ–≤–∞—è –∑–∞–ø–∏—Å—å"  # –î–ª—è –ø–µ—Ä–≤–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑—É
            cursor.execute("""
                INSERT INTO products (category, name, price, stock_status, timestamp, price_difference)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                category, name, new_price, stock_status, datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                str(price_difference)
            ))

            logging.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Ç–æ–≤–∞—Ä: {name} | –¶–µ–Ω–∞: {new_price} | –†–∞–∑–Ω–∏—Ü–∞: {price_difference}")

        conn.commit()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")
    finally:
        conn.close()


def get_pagination_links(base_url):
    try:
        response = requests.get(base_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        pagination = soup.find("div", class_="catalog-pagination__nums")

        if not pagination:
            logging.info(f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {base_url}. –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
            return [base_url]

        links = pagination.find_all("a", href=True)
        page_numbers = [int(link["href"].split("PAGEN_2=")[-1]) for link in links if "PAGEN_2=" in link["href"]]
        max_page = max(page_numbers) if page_numbers else 1
        return [f"{base_url}?PAGEN_2={page}" for page in range(1, max_page + 1)]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {e}")
        return [base_url]


def parse_wishmaster(url):
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        names = soup.find_all("div", class_="catalog-rounded-item__name")
        prices = soup.find_all("span", class_="catalog-rounded-item__price")
        stock_statuses = soup.find_all("div", class_="catalog-rounded-item__quantity-text")

        if not names or not prices or not stock_statuses:
            logging.warning(f"–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
            return []

        products = [
            (name.text.strip(), price.text.strip(), stock.text.strip())
            for name, price, stock in zip(names, prices, stock_statuses)
        ]

        logging.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)} –Ω–∞ {url}")
        return products
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return []


def parse_category(category_url, category_name):
    logging.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_name} ({category_url})")
    pages = get_pagination_links(category_url)
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    all_products = []
    for index, url in enumerate(pages, start=1):
        logging.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {index} ‚Üí {url}")
        products = parse_wishmaster(url)
        all_products.extend(products)
        time.sleep(DELAY_BETWEEN_REQUESTS)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞

    save_to_db(category_name, all_products)


if __name__ == "__main__":
    create_database()
    for url, category in categories.items():
        parse_category(url, category)

    logging.info("–í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑–µ!")
